{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chicago taxi transform load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessarry modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxi_trips_transformations(taxi_trips: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms a DataFrame containing taxi trip data by performing the following steps:\n",
    "    \n",
    "    1. Drops unnecessary columns: \"pickup_census_tract\", \"dropoff_census_tract\", \n",
    "       \"pickup_centroid_location\", and \"dropoff_centroid_location\".\n",
    "    2. Removes rows with missing values.\n",
    "    3. Renames columns \"pickup_community_area\" and \"dropoff_community_area\" to \n",
    "       \"pickup_community_area_id\" and \"dropoff_community_area_id\", respectively.\n",
    "    4. Adds a new column \"datetime_for_weather\", which is derived by rounding \n",
    "       the \"trip_start_timestamp\" column to the nearest hour.\n",
    "\n",
    "    Args:\n",
    "        taxi_trips (pd.DataFrame): The input DataFrame containing taxi trip data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed DataFrame with the specified modifications.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(taxi_trips, pd.DataFrame):\n",
    "        raise TypeError('taxi_trips is not a valid pandas DataFrame.')\n",
    "\n",
    "    taxi_trips.drop([\"pickup_census_tract\", \"dropoff_census_tract\",\n",
    "                     \"pickup_centroid_location\", \"dropoff_centroid_location\"], axis=1, inplace=True)\n",
    "\n",
    "    taxi_trips.dropna(inplace = True)\n",
    "\n",
    "    taxi_trips.rename(columns={\"pickup_community_area\": \"pickup_community_area_id\", \"dropoff_community_area\": \"dropoff_community_area_id\"}, inplace=True)\n",
    "\n",
    "    taxi_trips[\"datetime_for_weather\"] = pd.to_datetime(taxi_trips[\"trip_start_timestamp\"]).dt.floor(\"H\")\n",
    "\n",
    "    return taxi_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_taxi_trips_with_master_data(taxi_trips: pd.DataFrame, payment_type_master: pd.DataFrame, company_master: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Updates the taxi trips DataFrame by merging it with payment type and company master DataFrames to replace \n",
    "    textual payment type and company data with their corresponding IDs.\n",
    "\n",
    "    Args:\n",
    "        taxi_trips (pd.DataFrame): DataFrame containing taxi trip information, including 'payment_type' and 'company' columns.\n",
    "        payment_type_master (pd.DataFrame): DataFrame containing master data for payment types, with columns \n",
    "                                            'payment_type_id' and 'payment_type'.\n",
    "        company_master (pd.DataFrame): DataFrame containing master data for companies, with columns \n",
    "                                       'company_id' and 'company'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated taxi trips DataFrame where 'payment_type' and 'company' columns are replaced with \n",
    "                      'payment_type_id' and 'company_id' columns from the respective master DataFrames.\n",
    "    \"\"\"\n",
    "    taxi_trips_id = taxi_trips.merge(payment_type_master, on = 'payment_type')\n",
    "\n",
    "    taxi_trips_id = taxi_trips_id.merge(company_master, on = 'company')\n",
    "\n",
    "    taxi_trips_id.drop(['payment_type', 'company'], axis = 1, inplace = True)\n",
    "\n",
    "    return taxi_trips_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_master(taxi_trips: pd.DataFrame, master: pd.DataFrame, id_column: str, value_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Updates a master DataFrame by adding new values found in the taxi trips DataFrame.\n",
    "\n",
    "    Args:\n",
    "        taxi_trips (pd.DataFrame): DataFrame containing taxi trip information, including the column with new values.\n",
    "        master (pd.DataFrame): DataFrame representing the master data to be updated, including specified ID and value columns.\n",
    "        id_column (str): Name of the column in the master DataFrame representing unique identifiers.\n",
    "        value_column (str): Name of the column in the master DataFrame containing the values to be checked and updated.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated master DataFrame with new values added. New values are assigned unique IDs starting \n",
    "                      from the maximum value in the specified ID column of the existing master DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    max_id = master[id_column].max()\n",
    "\n",
    "    new_values_list = [value for value in taxi_trips[value_column].values if value not in master[value_column].values]\n",
    "\n",
    "    new_values_df = pd.DataFrame({\n",
    "    id_column : range(max_id + 1, max_id + len(new_values_list) + 1),\n",
    "    value_column : new_values_list\n",
    "    })\n",
    "\n",
    "    updated_master = pd.concat([master, new_values_df], ignore_index = True)\n",
    "\n",
    "    return updated_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_weather_data(weather_data: json) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms weather data in JSON format into a structured pandas DataFrame for easier analysis.\n",
    "\n",
    "    Args:\n",
    "        weather_data (json): JSON object containing hourly weather data, including time, temperature, wind speed, \n",
    "                             rain, and precipitation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the following columns:\n",
    "                      - 'datetime': Timestamps converted to datetime objects.\n",
    "                      - 'tempretaure': Hourly temperatures (in Â°C or relevant unit).\n",
    "                      - 'wind_speed': Hourly wind speed (in m/s or relevant unit).\n",
    "                      - 'rain': Hourly rainfall data.\n",
    "                      - 'precipitation': Hourly precipitation data.\n",
    "    \"\"\"\n",
    "    weather_data_filtered = {\n",
    "        \"datetime\": weather_data[\"hourly\"][\"time\"],\n",
    "        \"tempretaure\": weather_data[\"hourly\"][\"temperature_2m\"],\n",
    "        \"wind_speed\": weather_data[\"hourly\"][\"wind_speed_10m\"],\n",
    "        \"rain\": weather_data[\"hourly\"][\"rain\"],\n",
    "        \"precipitation\": weather_data[\"hourly\"][\"precipitation\"],\n",
    "    }\n",
    "\n",
    "    weather_df = pd.DataFrame(weather_data_filtered)\n",
    "\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"datetime\"])\n",
    "\n",
    "    return weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_s3(bucket: str, path: str, filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file from an S3 bucket and returns its contents as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        bucket (str): The name of the S3 bucket where the file is stored.\n",
    "        path (str): The path or prefix within the bucket where the file is located.\n",
    "        filename (str): The name of the file to be read.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the content of the CSV file.\n",
    "\n",
    "    Raises:\n",
    "        botocore.exceptions.ClientError: If there is an error accessing the S3 bucket or file.\n",
    "        UnicodeDecodeError: If the file cannot be decoded as UTF-8.\n",
    "        pd.errors.EmptyDataError: If the file is empty or cannot be parsed as CSV.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    full_path = f'{path}{filename}'\n",
    "\n",
    "    object = s3.get_object(Bucket = bucket, Key = full_path)\n",
    "    object = object['Body'].read().decode('utf-8')\n",
    "    output_df = pd.read_csv(StringIO(object))\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataframes_to_s3(bucket: str, path: str, dataframe: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Uploads a pandas DataFrame as a CSV file to an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        bucket (str): The name of the S3 bucket where the CSV file will be uploaded.\n",
    "        path (str): The file path in the S3 bucket, including the file name and extension (e.g., \"folder/file.csv\").\n",
    "        dataframe (pd.DataFrame): The pandas DataFrame to be uploaded.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    buffer = StringIO()\n",
    "    dataframe.to_csv(buffer, index = False)\n",
    "    df_content = buffer.getvalue()\n",
    "    s3.put_object(Bucket = bucket, Key = path, Body = df_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_master_data_to_s3(bucket: str, path: str, file_type: str, dataframe: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Uploads a master data file to an S3 bucket, replacing the existing master file and backing up\n",
    "    the previous version of the file.\n",
    "\n",
    "    Args:\n",
    "        bucket (str): The name of the S3 bucket where the master data file will be stored.\n",
    "        path (str): The directory path in the S3 bucket where the master data file is located.\n",
    "        file_type (str): The type or name prefix of the master data file (e.g., \"company\", \"payment_type\").\n",
    "        dataframe (pd.DataFrame): The pandas DataFrame containing the data to be uploaded.\n",
    "\n",
    "    Functionality:\n",
    "        1. Backs up the current master data file by copying it to a \"previous version\" directory in S3.\n",
    "        2. Converts the provided pandas DataFrame to a CSV format and uploads it to the master file location in S3.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    master_file_path = f'{path}{file_type}_master.csv'\n",
    "    previous_master_file_path = f'transformed_data/master_table_previous_version/{file_type}_master_previous_version.csv'\n",
    "\n",
    "    s3.copy_object(\n",
    "        Bucket = bucket,\n",
    "        CopySource = {'Bucket': bucket, 'Key': master_file_path},\n",
    "        Key = previous_master_file_path\n",
    "    )\n",
    "\n",
    "    upload_dataframes_to_s3(bucket = bucket, path = master_file_path, dataframe = dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_move_file_on_s3(\n",
    "        dataframe: pd.DataFrame,\n",
    "        datetime_column: str,\n",
    "        bucket: str,\n",
    "        file_type: str,\n",
    "        filename: str,\n",
    "        source_path: str,\n",
    "        target_path_raw: str,\n",
    "        target_path_transformed: str,\n",
    "    ):\n",
    "    \"\"\"\n",
    "        Uploads a pandas DataFrame to an S3 bucket, moves a specified file to a new path, \n",
    "    and deletes the original file from the source path.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The pandas DataFrame to be uploaded as a CSV file.\n",
    "        datetime_column (str): The column in the DataFrame containing datetime values for file naming.\n",
    "        bucket (str): The name of the S3 bucket.\n",
    "        file_type (str): The prefix or type of the file (e.g., \"weather\", \"taxi\").\n",
    "        filename (str): The name of the file to be moved.\n",
    "        source_path (str): The path in the S3 bucket where the original file is located.\n",
    "        target_path_raw (str): The path in the S3 bucket where the original file will be copied.\n",
    "        target_path_transformed (str): The path in the S3 bucket where the DataFrame will be uploaded.\n",
    "\n",
    "    Functionality:\n",
    "        1. Creates a new file name based on the datetime column's value and uploads the DataFrame as a CSV.\n",
    "        2. Moves the original file from the source path to the target raw path.\n",
    "        3. Deletes the original file from the source path after it has been moved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    formatted_date = dataframe[datetime_column].iloc[0].strftime(\"%Y-%m-%d\")\n",
    "    new_path_with_filename = f'{target_path_transformed}{file_type}_{formatted_date}.csv'\n",
    "\n",
    "    upload_dataframes_to_s3(bucket = bucket, path = new_path_with_filename, dataframe = dataframe)\n",
    "    \n",
    "    s3.copy_object(\n",
    "        Bucket = bucket,\n",
    "        CopySource = {'Bucket': bucket, 'Key': f'{source_path}{filename}'},\n",
    "        Key = f'{target_path_raw}{filename}'\n",
    "    )\n",
    "\n",
    "    s3.delete_object(Bucket = bucket, Key = f'{source_path}{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_from_s3(bucket: str, key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads a JSON file from an S3 bucket and returns its content as a Python dictionary.\n",
    "\n",
    "    Args:\n",
    "        bucket (str): The name of the S3 bucket where the JSON file is located.\n",
    "        key (str): The key (file path) of the JSON file within the bucket.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the parsed contents of the JSON file.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    response = s3.get_object(Bucket = bucket, Key = key)\n",
    "    content = response[\"Body\"]\n",
    "    output_dict = json.loads(content.read())\n",
    "\n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_handler(event, context):\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    bucket = 'cubix-chicago-taxi-ld'\n",
    "\n",
    "    raw_taxi_trips_folder = 'raw_data/to_processed/taxi_data/'\n",
    "    raw_weather_folder = 'raw_data/to_processed/weather_data/'\n",
    "    target_taxi_trips_folder = 'raw_data/processed/taxi_data/'\n",
    "    target_weather_folder = 'raw_data/processed/weather_data/'\n",
    "\n",
    "    transformed_taxi_trips_folder = 'transformed_data/taxi_trips/'\n",
    "    transformed_weather_folder = 'transformed_data/weather/'\n",
    "    \n",
    "    payment_type_master_folder = 'transformed_data/payment_type/'\n",
    "    payment_type_master_filename = 'payment_type_master.csv'\n",
    "    \n",
    "    company_master_folder = 'transformed_data/company/'\n",
    "    company_master_filename = 'company_master.csv'\n",
    "\n",
    "    payment_type_master = read_csv_from_s3(bucket = bucket, path = payment_type_master_folder, filename = payment_type_master_filename)\n",
    "    company_master = read_csv_from_s3(bucket = bucket, path = company_master_folder, filename = company_master_filename)\n",
    "\n",
    "    # Taxi data transformation and loading\n",
    "    for file in s3.list_objects(Bucket = bucket, Prefix = raw_taxi_trips_folder)['Contents']: # list out the meta data of files in taxi_data folder\n",
    "        taxi_trip_key = file['Key'] # key is the part of the json file we need\n",
    "\n",
    "        if taxi_trip_key.split('/')[-1].strip() != '': # split the strings by '/' to filter only the file names\n",
    "            if taxi_trip_key.split('.')[1] == 'json': # split the strings by '.' to filter only the json files\n",
    "\n",
    "                filename = taxi_trip_key.split('/')[-1]\n",
    "\n",
    "                taxi_trips_data_json = read_json_from_s3(bucket = bucket, key = taxi_trip_key) # transform the json file into a dictionary\n",
    "\n",
    "                taxi_trips_data_raw = pd.DataFrame(taxi_trips_data_json) # transform the dictionary into a dataframe\n",
    "                taxi_trips_transformed = taxi_trips_transformations(taxi_trips = taxi_trips_data_raw) # transform the dataframe\n",
    "\n",
    "                company_master_updated = update_master(taxi_trips = taxi_trips_transformed, master = company_master, id_column = 'company_id', value_column = 'company')\n",
    "                payment_type_master_updated = update_master(taxi_trips = taxi_trips_transformed, master = payment_type_master, id_column = 'payment_type_id', value_column = 'payment_type')\n",
    "\n",
    "                taxi_trips = update_taxi_trips_with_master_data(taxi_trips = taxi_trips_transformed, payment_type_master = payment_type_master_updated, company_master = company_master_updated)\n",
    "\n",
    "                upload_and_move_file_on_s3(\n",
    "                    dataframe = taxi_trips,\n",
    "                    datetime_column = 'datetime_for_weather',\n",
    "                    bucket = bucket,\n",
    "                    file_type = 'taxi',\n",
    "                    filename = filename,\n",
    "                    source_path = raw_taxi_trips_folder,\n",
    "                    target_path_raw = target_taxi_trips_folder,\n",
    "                    target_path_transformed = transformed_taxi_trips_folder\n",
    "                    )\n",
    "                print('taxi_trips is uploaded and moved.')\n",
    "                                \n",
    "                upload_master_data_to_s3(bucket = bucket, path = payment_type_master_folder, file_type = 'payment_type', dataframe = payment_type_master_updated)\n",
    "                print('payment_type_master has been updated.')\n",
    "                upload_master_data_to_s3(bucket = bucket, path = company_master_folder, file_type = 'company', dataframe = company_master_updated)\n",
    "                print('company_master has been updated.')\n",
    "\n",
    "    # Weather data transformation and loading\n",
    "    for file in s3.list_objects(Bucket = bucket, Prefix = raw_weather_folder)['Contents']: # list out the meta data of files in weather_data folder\n",
    "        weather_key = file['Key'] # key is the part of the json file we need\n",
    "\n",
    "        if weather_key.split('/')[-1].strip() != '': # split the strings by '/' to filter only the file names\n",
    "            if weather_key.split('.')[1] == 'json': # split the strings by '.' to filter only the json files\n",
    "\n",
    "                filename = weather_key.split('/')[-1]\n",
    "\n",
    "                weather_data_json = read_json_from_s3(bucket = bucket, key = weather_key) # transform the json file into a dictionary\n",
    "                        \n",
    "                weather_data = transform_weather_data(weather_data_json) # transform the dictionary into a dataframe\n",
    "\n",
    "                upload_and_move_file_on_s3(\n",
    "                    dataframe = weather_data,\n",
    "                    datetime_column = 'datetime',\n",
    "                    bucket = bucket,\n",
    "                    file_type = 'weather',\n",
    "                    filename = filename,\n",
    "                    source_path = raw_weather_folder,\n",
    "                    target_path_raw = target_weather_folder,\n",
    "                    target_path_transformed = transformed_weather_folder\n",
    "                    )\n",
    "                print('weather_data is uploaded and moved.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
